# General
evaluate: False # False -> train the model
resume: 

# Paths
data_path: data/
output_path: output/

# Dataset 
dataset: cifar10
transforms: randaugment # none | standard | cutout | autoaugment | cutout_autoaugment | randaugment | randaugment_cutout
randaug_include_cutout: True
randaug_N: 2 # Number of transforms to apply (RandAugment) 
randaug_M: 5 # Magnitude of transforms (RandAugment)

# Model 
arch: preactresnet18  # preactresnet18 | preactresnet34 | preactresnet50 | preactresnet101 | preactresnet152
initial_channels: 64
weight_init: normal # normal | xavier | kaiming | orthogonal
weight_init_gain: 0.02 # std

# Training 
training: vanilla # vanilla | mixup | manifold_mixup | pro_mixup
mixup_alpha: 
mixup_layers:  # Resblocks that mixup is ranomly choosen to happen between (Subset of [0,1,2,3])
smoothing: 0.0

batch_size: 128
workers: 6
lr: 0.1
momentum: 0.9
nesterov: True
weight_decay: 0.0001
  
# Scheduler
epochs: 200 
schedule: step # none | step 
steps: [100, 150] # Decrease lr at these epochs 
step_size: 0.1 # lr reduction per step 

# Logging
batch_log_rate: 50 # Number of batches between training progress prints

# GAN
use_gan: False
use_gan_freq:  # Per how many batchs is a gan batch used (eg. 1 = every batch is generated)
gan_weights:  
truncation: 
