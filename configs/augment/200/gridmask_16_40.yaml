# General
evaluate: False # False -> train the model
resume: 
seed: 

# Paths
data_path: data/
output_path: output/

# Dataset 
dataset: cifar10
transforms: gridmask # none | standard | cutout | autoaugment | autoaugment_cutout | randaugment | randaugment_cutout | gridmask | autoaugment_gridmask
randaug_include_cutout: # Include cutout as a randomly choosable transformation
randaug_N: # Number of transforms to apply (RandAugment) 
randaug_M: # Magnitude of transforms [0, 30] (RandAugment)
gridmask_r: 0.4
gridmask_minD: 16
gridmask_maxD: 40
gridmask_rotate: 360 # [0, 360]


# Model 
arch: preactresnet18  # preactresnet18 | preactresnet34 | preactresnet50 | preactresnet101 | preactresnet152
initial_channels: 64
weight_init: normal # normal | xavier | kaiming | orthogonal
weight_init_gain: 0.02 # std

# Training 
training: vanilla # vanilla | mixup | manifold_mixup | cutmix
mixup_alpha: 
mixup_layers:  # Resblocks that mixup is ranomly choosen to happen between (Subset of [0,1,2,3])
mix_prob:  # [0,1]
smoothing: 0 # [0,1]

# Optimizer
batch_size: 128
workers: 6
lr: 0.1
momentum: 0.9
nesterov: True
weight_decay: 0.0001
  
# Scheduler
epochs: 200 
schedule: step # none | step 
steps: [100, 150] # Decrease lr at these epochs 
step_size: 0.1 # lr reduction per step 


# Logging
batch_log_rate: 50 # Number of batches between training progress prints

# GAN
use_gan: False
use_gan_freq: 1 # Per how many batchs is a gan batch used (eg. 1 = every batch is generated)
gan_weights: gan/weights/gen_ema.pth 
truncation: 
