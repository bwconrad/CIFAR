# General
evaluate: False # False -> train the model
resume: 

# Paths
data_path: data/
output_path: output/

# Dataset 
dataset: cifar10
transforms: standard # none | standard | cutout | autoaugment | autoaugment_cutout | randaugment | randaugment_cutout
randaug_include_cutout: False # Include cutout as a randomly choosable transformation
randaug_N: 2 # Number of transforms to apply (RandAugment) 
randaug_M: 5 # Magnitude of transforms [0, 30] (RandAugment)


# Model 
arch: preactresnet18  # preactresnet18 | preactresnet34 | preactresnet50 | preactresnet101 | preactresnet152
initial_channels: 64
weight_init: normal # normal | xavier | kaiming | orthogonal
weight_init_gain: 0.02 # std

# Mixup 
training: manifold_cutmix # vanilla | mixup | manifold_mixup | cutmix | manifold_cutmix
mixup_alpha: 1 
mixup_layers: [0,1,2] # Resblocks that mixup is ranomly choosen to happen between (Subset of [0,1,2,3])
mix_prob: 1.0 # [0,1]
smoothing: 0

# Optimizer
batch_size: 128
workers: 6
lr: 0.1
momentum: 0.9
nesterov: True
weight_decay: 0.0001
  
# Scheduler
epochs: 600 
schedule: step # none | step 
steps: [300, 450] # Decrease lr at these epochs 
step_size: 0.1 # lr reduction per step 


# Logging
batch_log_rate: 50 # Number of batches between training progress prints

# GAN
use_gan: False
use_gan_freq: 1 # Per how many batchs is a gan batch used (eg. 1 = every batch is generated)
gan_weights: gan/weights/gen_ema.pth 
truncation: 
